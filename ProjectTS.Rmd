---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


```{r, echo=FALSE, message=FALSE}
library(tseries)
library(forecast)
```
Objectives
1.Summary of Analysis
2.Data history
3.model chosen and the results


The objective is to collect the sales data from a tractor manufacturing company, Powerhouse.  Help PowerHorse Tractors with forecasting sales for them to manage their inventories and suppliers.


```{r}
T_sales = read.csv('http://ucanalytics.com/blogs/wp-content/uploads/2015/06/Tractor-Sales.csv')
head(T_sales)
tail(T_sales)


```
Transform the data into a timeseries data

```{r}
salesData = ts(T_sales[,2],start = c(2003,1),frequency = 12)
head(salesData)
```

```{r}
plot(salesData)
```

```{r}
plot(decompose(salesData))
```

```{r}
seasonplot(salesData, 12, col=rainbow(12), year.labels=TRUE, main="Seasonal plot: Tractor sales") # seasonal frequency set as 12 for monthly data.
```




Clearly the above chart has an upward trend for tractors sales and there is also a seasonal component.

Step 2: 

The next thing to do is to make the series stationary  This to remove the upward trend through 1st order differencing the series using the following formula: to make the data stationary on the mean

1st Differencing (d=1)	 $ Y_{t}^{'}=Y_t -Y_{t-1} $


```{r}
plot(diff(salesData),ylab='Differenced Tractor Sales')
```

so the above series is not stationary on variance i.e. variation in the plot is increasing as we move towards the right of the chart. We need to make the series stationary on variance to produce reliable forecasts through ARIMA models



Step 3: log transform data to make data stationary on variance

 To make a series stationary on variance is through transforming the original series through log transform.  The following equation represents the process of log transformation mathematically:
 
 
```{r}
plot(log10(salesData),ylab='Log (Tractor Sales)')
```



Step 4: Difference log transform data to make data stationary on both mean and variance

Let us look at the differenced plot for log transformed series to reconfirm if the series is actually stationary on both mean and variance.

```{r}
plot(diff(log10(salesData)),ylab='Differenced Log (Tractor Sales)')


```


 This series looks stationary on both mean and variance. This also gives us the clue that I or integrated part of our ARIMA model will be equal to 1 as 1st difference is making the series stationary.



autocorrelation factor (ACF) and partial autocorrelation factor (PACF) plots to identify patterns in the above data which is stationary on both mean and variance. The idea is to identify presence of AR and MA components in the residuals. The following is the R code to produce ACF and PACF plots.

```{r}
par(mfrow = c(1,2))
acf(ts(diff(log10(salesData))),main='ACF Tractor Sales')
pacf(ts(diff(log10(salesData))),main='PACF Tractor Sales')

```
Since, there are enough spikes in the plots outside the insignificant zone (dotted horizontal lines) we can conclude that the residuals are not random. This implies that there is juice or information available in residuals to be extracted by AR and MA models. Also, there is a seasonal component available in the residuals at the lag 12 (represented by spikes at lag 12). This makes sense since we are analyzing monthly data that tends to have seasonality of 12 months because of patterns in tractor sales.



```{r}
salesfit = auto.arima(log10(salesData), approximation=FALSE,trace=FALSE)
summary(salesfit)
```


he best fit model is selected based on Akaike Information Criterion (AIC) , and Bayesian Information Criterion (BIC) values. The idea is to choose a model with minimum AIC and BIC values.


As expected, our model has I (or integrated) component equal to 1. This represents differencing of order 1. There is additional differencing of lag 12 in the above best fit model. Moreover, the best fit model has MA value of order 1. Also, there is seasonal MA with lag 12 of order 1.


Step 6: Forecast sales using the best fit ARIMA model

The next step is to predict tractor sales for next 3 years i.e. for 2015, 2016, and 2017 through the above model. 

```{r}
pred = predict(salesfit, n.ahead = 36)
pred

```



```{r}
plot(salesData,type='l',xlim=c(2004,2018),ylim=c(1,1600),xlab = 'Year',ylab = 'Tractor Sales')
lines(10^(pred$pred),col='blue')
lines(10^(pred$pred+2*pred$se),col='orange')
lines(10^(pred$pred-2*pred$se),col='orange')
```


Now, forecasts for a long period of 3 years is an ambitious task. The major assumption here is that the underlining patterns in the time series will continue to stay the same as predicted in the model. A short-term forecasting model, say a couple of business quarters or a year, is usually a good idea to forecast with reasonable accuracy. A long-term model like the one above needs to evaluated on a regular interval of time (say 6 months). The idea is to incorporate the new information available with the passage of time in the model.


Step 7: Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction

Finally, letâ€™s create an ACF and PACF plot of the residuals of our best fit ARIMA model i.e. ARIMA(0,1,1)(0,1,1)[12]. The following is the R code for the same.

```{r}
par(mfrow=c(1,2))
acf(ts(salesfit$residuals),main='ACF Residual')
pacf(ts(salesfit$residuals),main='PACF Residual')

```

Since there are no spikes outside the insignificant zone for both ACF and PACF plots we can conclude that residuals are random with no information or juice in them. Hence our ARIMA model is working fine.




